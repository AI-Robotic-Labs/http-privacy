#include "privacy_http_sdk/src/lib.rs.h" // Auto-generated by the `cxx` crate
#include <iostream>
#include <unordered_map>
#include <thread>
#include <nlohmann/json.hpp>
#include <pybind11/pybind11.h>
#include <pybind11/embed.h>
#include <httplib.h>
#include <fstream>
#include <string>

using json = nlohmann::json;
namespace py = pybind11;

// Python code to run Stable Diffusion (unchanged)
std::string run_stable_diffusion(const std::string& prompt, int width, int height, int steps) {
    py::scoped_interpreter guard{};
    try {
        auto module = py::module_::import("sys");
        module.attr("path").attr("append")(".");
        std::string code = R"(
import torch
from diffusers import StableDiffusionPipeline
import base64
from io import BytesIO
from PIL import Image

def generate_image(prompt, width, height, steps):
    pipe = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float16,
        use_auth_token=False
    ).to("cuda" if torch.cuda.is_available() else "cpu")
    
    image = pipe(
        prompt,
        width=width,
        height=height,
        num_inference_steps=steps
    ).images[0]
    
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")
)";
        auto locals = py::dict();
        py::exec(code, py::globals(), locals);
        auto generate_image = locals["generate_image"];
        return generate_image(prompt, width, height, steps).cast<std::string>();
    } catch (const py::error_already_set& e) {
        throw std::runtime_error("Python error: " + std::string(e.what()));
    }
}

// Process MCP messages
json process_mcp_message(const json& message) {
    try {
        std::string message_type = message.at("message_type").get<std::string>();
        std::string sender = message.at("sender").get<std::string>();
        json payload = message.at("payload");

        // Example processing based on message_type
        if (message_type == "command") {
            std::string action = payload.at("action").get<std::string>();
            std::string data = payload.at("data").get<std::string>();
            // Example: Echo the action and data
            return {
                {"status", "success"},
                {"response", "Received command from " + sender + ": " + action + ", data: " + data},
                {"message_id", "123"} // Placeholder ID
            };
        } else if (message_type == "query") {
            // Example: Handle a query
            return {
                {"status", "success"},
                {"response", "Query processed for " + sender},
                {"message_id", "124"}
            };
        } else {
            return {
                {"status", "error"},
                {"response", "Unknown message_type: " + message_type},
                {"message_id", "0"}
            };
        }
    } catch (const std::exception& e) {
        return {
            {"status", "error"},
            {"response", "Invalid MCP message: " + std::string(e.what())},
            {"message_id", "0"}
        };
    }
}

// Start the local API server with MCP endpoint
void start_api_server() {
    httplib::Server svr;

    // Existing Stable Diffusion endpoint
    svr.Post("/txt2img", [](const httplib::Request& req, httplib::Response& res) {
        try {
            json payload = json::parse(req.body);
            std::string prompt = payload["prompt"].get<std::string>();
            int width = payload["width"].get<int>();
            int height = payload["height"].get<int>();
            int steps = payload["steps"].get<int>();

            if (prompt.find("<script") != std::string::npos || prompt.find("..") != std::string::npos) {
                res.set_content("Invalid prompt", "text/plain");
                res.status = 400;
                return;
            }

            std::string image_base64 = run_stable_diffusion(prompt, width, height, steps);
            json response = {{"image", image_base64}};
            res.set_content(response.dump(), "application/json");
        } catch (const std::exception& e) {
            res.set_content("Error: " + std::string(e.what()), "text/plain");
            res.status = 500;
        }
    });

    // New MCP endpoint
    svr.Post("/mcp", [](const httplib::Request& req, httplib::Response& res) {
        try {
            json message = json::parse(req.body);
            json response = process_mcp_message(message);
            res.set_content(response.dump(), "application/json");
        } catch (const std::exception& e) {
            json error = {
                {"status", "error"},
                {"response", "Failed to process MCP message: " + std::string(e.what())},
                {"message_id", "0"}
            };
            res.set_content(error.dump(), "application/json");
            res.status = 400;
        }
    });

    std::cout << "Starting API server on http://127.0.0.1:8080" << std::endl;
    std::cout << "MCP endpoint available at http://127.0.0.1:8080/mcp" << std::endl;
    svr.listen("127.0.0.1", 8080);
}

// Extend the privacy_http_sdk client interface (unchanged)
namespace privacy_http_sdk {
    void generate_image(
        HttpClient& client,
        const std::string& prompt,
        int width,
        int height,
        int steps,
        const std::string& output_path
    ) {
        std::string url = "http://127.0.0.1:8080/txt2img";
        std::unordered_map<std::string, std::string> headers = {
            {"Content-Type", "application/json"}
        };
        json payload = {
            {"prompt", prompt},
            {"width", width},
            {"height", height},
            {"steps", steps}
        };
        std::string body = payload.dump();
        std::string response = client.post(url, headers, body);
        json response_json = json::parse(response);
        std::string image_data = response_json["image"].get<std::string>();
        std::string decoded = image_data; // TODO: Implement base64 decoding
        std::ofstream file(output_path, std::ios::binary);
        if (!file) {
            throw std::runtime_error("Failed to open output file: " + output_path);
        }
        file.write(decoded.c_str(), decoded.size());
        file.close();
    }
}

int main() {
    // Start the API server (with MCP endpoint) in a separate thread
    std::thread server_thread(start_api_server);

    // Wait briefly to ensure server starts
    std::this_thread::sleep_for(std::chrono::seconds(2));

    // Initialize the privacy_http_sdk client
    auto client = privacy_http_sdk::new_http_client();

    std::cout << "PrivacyHttpSdk Version: " << PRIVACY_HTTP_SDK_VERSION << std::endl;

    // Test the MCP endpoint
    try {
        std::string mcp_url = "http://127.0.0.1:8080/mcp";
        std::unordered_map<std::string, std::string> headers = {
            {"Content-Type", "application/json"}
        };
        json mcp_message = {
            {"message_type", "command"},
            {"sender", "test_client"},
            {"payload", {
                {"action", "test_action"},
                {"data", "test_data"}
            }}
        };
        std::string mcp_body = mcp_message.dump();
        auto mcp_response = client->post(mcp_url, headers, mcp_body);
        std::cout << "MCP Response: " << mcp_response << std::endl;
    } catch (const std::exception& e) {
        std::cerr << "MCP Error: " << e.what() << std::endl;
    }

    // Existing GET/POST requests (unchanged)
    std::unordered_map<std::string, std::string> headers = {
        {"Authorization", "Bearer YOUR_API_KEY"},
        {"Content-Type", "application/json"}
    };
    std::vector<std::string> urls = {
        "https://api.openai.com/v1/models",
        "https://api.gemini.google.com/v1/models",
        "https://api.deepseek.com",
        "https://bedrock-runtime.us-east-1.amazonaws.com",
        "https://api.x.ai/v1/models",
        "https://api.x.ai/v1", // Grok 3 API
        "https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions",
        "https://api.anthropic.com/v1/messages",
        "http://localhost:11434/api/generate"
    };
    for (const auto& url : urls) {
        try {
            auto response = client->get(url, headers);
            std::cout << "GET Response from " << url << ": " << response << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "GET Error from " << url << ": " << e.what() << std::endl;
        }
    }
    try {
        std::string body = R"({"prompt": "Hello, world!", "max_tokens": 5})";
        auto response = client->post("https://api.openai.com/v1/completions", headers, body);
        std::cout << "POST Response: " << response << std::endl;
    } catch (const std::exception& e) {
        std::cerr << "POST Error: " << e.what() << std::endl;
    }

    // Perform Stable Diffusion image generation (unchanged)
    try {
        privacy_http_sdk::generate_image(*client, "A serene landscape", 512, 512, 50, "output.png");
        std::cout << "Image saved to output.png" << std::endl;
    } catch (const std::exception& e) {
        std::cerr << "Stable Diffusion Error: " << e.what() << std::endl;
    }

    server_thread.join();
    return 0;
}